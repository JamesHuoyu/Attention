---
markdown: mathjax
---
# Attention 单卡优化实验报告
## 优化步骤和简介
本次实验的目标是构建opt.cu代码对原有的Attention计算机制进行优化，使得其在单卡上的性能得到提升。

本次实验的平台为：A40,对应8.6的计算能力，可用memory为46068 MiB，理论峰值性能为37.4TFlop/s。

在框架的实现过程中，主要采取了以下优化手段：

1.编译器选项优化

3.数据重用：通过调整数据的存储方式和访问方式，减少数据的读取次数。

4.数据预取调整：通过调整数据的预取，减少数据的读取次数，提高流水线的效率。

5.kernel架构调整修改：通过重构合并计算方法，减少访存影响，提高带宽。

首先在opt.cu中对照naive.c搭建以4个kernel驱动的Attention计算过程。
### 具体优化过程：
1.增加`-O4 --use_fast_math -arch=sm_86
`的编译指导语句，性能有了明显提高，但相比CPU单核优化实现过程，性能提高程度不明显。

分析可知，带宽是影响当前程序性能的最重要的限制。以及Sgemm乘法在当前实现过程中的HotSpots，因此重点考虑Sgemm乘法的具体优化实现：
#### Sgemm乘法计算优化：
##### Shared-memory使用：
在naive版本的实现上，使用shared-memory对矩阵进行切片分块，加载到shared-memory中，保证数据的复用率，提高带宽：

（1）更改block分布为2维：(TILE, TILE), 沿K轴进行切片，每次加载TILE的大小，进而加载TILE*TILE大小的矩阵块进入shared-memory进行复用计算。

性能明显提高。

（2）当前实现过程中对于线程的使用过于保守，计算访存比低，每进行两次load操作后才能进行一次FMA指令，计算访存比约只有1/3，因此考虑每个线程加载4个数据，并进行计算,此时在寄存器中每进行`2*4`次load操作后进行`4*4`次FMA,计算指令占总指令数的2/3。

性能明显提高。

（3）考虑到Occupation和寄存器数量的限制，计算访存比不能一直提高，同时考虑到shared-memory的大小，我在本次实验中采用分段测率只固定TILE的大小为32和64，每个线程加载4个数据作为const variable。当n大于128时采用TILE = 64的较大数据块加载。

性能明显提高。

（4）由于从smem到reg的过程中，一次性load 4个数据，如果使用`float4`的数据类型，可以对访存进行高效实现。因此重构从global memory -> shared memory -> reg的访问和存储，其中，主要问题在于全局数据的对齐问题：进行全局数据索引时，对应地址偏移为`（row * n + col）`，其中由于在实验过程中n不一定为4的倍数，这是导致加载过程中不能完全对齐使用向量化的瓶颈。
1. 首先尝试padding的方式进行重构，扩充成4的倍数，该方法会显著降低性能。分析得之，如果我们在从HostToDevice过程中可以使用padding或者对齐的方法的话，并不会造成如此明显的全局内存访问和索引O(n^2)的复杂度，但如果在opt.cu程序中经过DeviceToDevice操作
便会造成立即的读写开销，不仅影响访存，也会造成流水线并行度的降低。
2. 为了保证数据访存的正确性，最终采用分支预测和三元运算的方式分类进行数据加载，在地址对齐的情况下采用float4的直接加载，而在地址不对齐的情况下，仍采用标量加载的方式。

性能得到明显提高。

(5) 同样的float4的方法用于计算和存储中，尽可能将原本的标量操作在正确性的前提下转变成向量化加载。

性能明显提高。

(6)进行数据的合并访问考虑：
1. 在Attention的实现过程中涉及矩阵的转置，如何对矩阵尽可能进行友好的合并访问，对访问性能的影响是重要的。对此，将原来实现过程中的显式转置转化成隐式转置，在reg加载的过程中再进行转置访问操作，可以帮助减小在访问过程中可能存在的stride。
```c
for TILE_block in k_TILES:
    smemB[row_in_block][col_in_block] = B[global_row][global_col];
// do something from smem to reg
```

性能明显提高。

2. 进一步进行矩阵的合并访问考虑，在当前实现下，`A @ B`时对于B从smem到reg的访问时连续简单的，系统可以进行合并访问：
```c
for k in k_TILE:
    reg_B = smemB[k][threadIdx.x]
```

但是对于A的访问以及`A @ B_T`中B_T的访问，则需要跨stride进行列访问，对此，同样采用隐式转置的方式进行访问，缩减一个原本存在的for循环为：
```c
for k in k_TILE:
    reg_A = smemA[k][threadIdx.y]
```

性能明显提高。

(7)Bank Conflict分析：通过Nsight Compute进行分析，发现此时在Sgemm中存在大量的Bank Conflict现象。分析当前的Warp组织方式：在TILE = 64的情况下，一个Block中有8个Warp,在第一个Warp内threadIdx.x 从0~15，threadIdx.y 从0~1,而一个float4类型的加载需要跨4个Bank,32个Bank对应8个线程的同时加载。在(row * n + col)的地址索引模式下，会存在明显的Bank Conflict,如0号线程和8号线程，1号线程和9号线程会访问同一个Conflict而导致串行化，影响系统的性能表现。
对此，首先改变Warp的组织方式,分割block的大小为（TILE/8，TILE/2），对应在一个Warp内的threadIdx.x 从0~7，threadIdx.y 从0~3,此时保证在同一行threadRow的索引下的8个线程不会发生Bank Conflict冲突。
其次，改变shared-memory的存储大小，由于修改了float4的类型，当前的smem对应索引为`smem[threadRow][threadIdx.x]`，大小为:`TILE * TILE / 8`，在访问时存在(threadRow * 8 + threadIdx.x)的形式，考虑此时的`threadRow = 2 * threadIdx.y`在一个Warp中为0~6，在固定threadIdx.x的情况下，同一Warp列中的线程会发生明显的Bank Conflict,因此采用修改smem大小为`TILE * （TILE / 8 + 1)`错开每一列中的线程对应bank索引。

性能明显提高,使用Nsight Compute进行分析可知，在小规模和中等规模的矩阵中已经不会或很少会出现Bank Conflict,在大规模的矩阵中Bank Conflict数下降2~3个数量级。

(8)在尽可能提高带宽，优化访存之后，考虑在当下latency的情况下进行优化和掩盖。对此，采用双缓冲技术，预取下一块smem内存，并对当前内存块进行计算，异步进行计算和访存，尽可能掩盖访存所带来的开销。

性能在大规模矩阵（矩阵规模大于4000）时性能有明显提高，而对小规模和中等规模矩阵引入更大的开销。故进一步分段，当矩阵规模大于4000时才使用双缓冲技术。

#### Softmax和Scale优化：
在naive版本的Softmax实现过程中，对于中间矩阵`QK_T`进行了最大值遍历，expf遍历，两次全局过程的加载，对性能影响较大。对此采用OnlineSoftmax思想：同时遍历更新最大值和expf局部求和和分布，将全局遍历的访存以增加计算量的方式进行掩盖：
```c
// in every thread
 find_max()
 calculate_expf_sum()
 calculate_distribution()
// in every warp
 find_max()
 generate_expf_sum()
 generate_expf_distribution()
// in every block
// globaly refresh

// here the generate_...:
    new_max = max(old_max, cur_max)
    expf_sum = old_expf_sum * expf(old_max - new_max) + cur_expf_sum * expf(cur_max - new_max)
```
性能明显提高

在Scale中，对中间矩阵`QK_T`进行了一次全局的快速读取放缩和存储,这影响了性能表现且是无意义的。
因此，将全局Scale纳入Sgemm的写回步骤中，减少一次全局访问和存储。

性能明显提高。
#### 后续思考：
考虑到如果完全按照Flash Attention的思路合并Softmax和Sgemm乘法操作，对当前代码的改造过大，且需重构sgemm算法内核(目前两次Sgemm算法无法直接合并)并考虑smem以及softmax共享内存在同一个kernel中在TILE = 64的情况下，可能会过大而导致崩溃。因而工作量过大，在本次实验中不予考虑实现。

最后经过模板化，合并等操作减少当前代码量，得到一个性能与复杂度平衡的实现作为最终版本。
### 当前性能分析
![性能表现](./Attention优化/图片1.png)
在NVIDIA A40平台上，该CUDA实现的性能表现可归类为**中等偏上水平**，具体分析如下：

---

#### **1. 理论性能基准**
- **A40单精度浮点（FP32）理论峰值**：约 **37.4 TFLOP/s**。
- **显存带宽**：696 GB/s（GDDR6 384-bit总线）。

---

#### **2. 实际性能表现**
##### **(1) 大矩阵场景（如4096×4096）**
- **观测性能**：26 TFLOP/s（接近理论峰值的 **~70%**）。
- **评价**：  
  - 表明代码在大规模计算中充分利用了GPU的计算单元和内存带宽，优化效果优秀。

##### **(2) 小矩阵场景（如64×64）**
- **观测性能**：~10 GFLOP/s（理论峰值的 **~0.03%**）。
- **评价**：  
  - 性能极低，主要由核函数启动开销、线程块配置不当、内存访问不连续导致。  

##### **(3) 平均性能**
- **平均GFLOP/s**：11.585 TFLOP/s（理论峰值的 **~30%**）。  
- **评价**：  
  - 中等水平，表明代码在混合尺寸任务中存在明显瓶颈（如Softmax核函数、小矩阵效率拖累整体）。
---

#### **5. 性能等级总结**
| 场景           | 性能水平               | 评价                          |
|----------------|-----------------------|-----------------------------|
| **大矩阵**     | **优秀**（接近70%理论） | 与cuBLAS相当，优化效果显著。     |
| **小矩阵**     | **较差**（<1%理论）    | 需针对性优化线程块和内存访问。    |
| **平均性能**   | **中等**（30%理论）    | 混合任务中规中矩，仍有提升空间。  |

---

**结论**：在A40平台上，该实现**大矩阵性能优秀**，接近商业库水平；**小矩阵性能严重不足**，拖累整体表现。但考虑到Attention的实现一般基于较大规模的矩阵，如果我们考虑`1024`之后大小的矩阵，该程序的平均性能可以达到`17.312TFlop/s`,约为峰值性能的47%,是一个良好的实现。